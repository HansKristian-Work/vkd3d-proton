#version 450
#extension GL_KHR_shader_subgroup_basic : require
#extension GL_KHR_shader_subgroup_ballot : require
#extension GL_KHR_shader_subgroup_arithmetic : require
#extension GL_KHR_shader_subgroup_shuffle : require
#extension GL_KHR_shader_subgroup_vote : require
#extension GL_EXT_buffer_reference : require
#extension GL_GOOGLE_include_directive : require

layout(local_size_x_id = 0) in;
layout(constant_id = 1) const uint FUSED_DIVIDER = 0;

#include "cs_workgraph_data_structures.h"

layout(buffer_reference, buffer_reference_align = 8, std430) buffer NodeAtomics
{
	uint payload_atomic;
	layout(offset = 8) NodeCounts node_counts[];
};

// Sharing across threads requires coherent, even within a subgroup.
layout(buffer_reference, buffer_reference_align = 16, std430) coherent buffer IndirectCommandsBuffer
{
	IndirectCommands indirect_commands[];
};

layout(buffer_reference, buffer_reference_align = 4, std430) readonly buffer DividersOrAmplification
{
	int data[];
};

layout(buffer_reference, buffer_reference_align = 4, std430) readonly buffer NodeShareMapping
{
	uint data[];
};

layout(push_constant, std430) uniform Registers
{
	NodeAtomics atomics;
	IndirectCommandsBuffer commands;
	DividersOrAmplification dividers;
	NodeShareMapping node_share_mapping;
	uint num_nodes;
} registers;

// Assumption is that number of nodes is fairly small and can reasonably be iterated over one wave.
// We do very little work here, just parcel out memory regions and have fun.
// Here, we could in theory detect OOM, report the failure and nop out the indirects.

const uint WG_DIVIDER = 32 * 1024;

void main()
{
	// Just in case the driver is being annoying.
	if (gl_SubgroupID != 0)
		return;

	uint linear_offset = 0;
	bool has_share_input = false;

	for (uint i = 0; i < registers.num_nodes; i += gl_SubgroupSize)
	{
		uint node_index = i + gl_SubgroupInvocationID;
		NodeCounts counts = NodeCounts(0, 0);
		uint total_wgs;

		if (node_index < registers.num_nodes)
		{
			uint sharing_index = registers.node_share_mapping.data[node_index];
			if (sharing_index == ~0u)
			{
				counts = registers.atomics.node_counts[node_index];
				total_wgs = counts.total;
			}
			else
			{
				// Don't contribute to the prefix sum.
				// It is not allowed for a node to be both a real input while also sharing input of another node.
				NodeCounts share_counts = registers.atomics.node_counts[sharing_index];
				total_wgs = share_counts.total;
				has_share_input = true;
			}
		}

		uint scan = subgroupInclusiveAdd(counts.total);
		uint total_scan = subgroupShuffle(scan, gl_SubgroupSize - 1);
		scan -= counts.total;

		if (node_index < registers.num_nodes)
		{
			uint node_linear_offset = scan + linear_offset;
			int coalesce_divider = registers.dividers.data[node_index];

			// Could make this multiplier-based if we need to.
			if (coalesce_divider > 0)
				total_wgs = (total_wgs + uint(coalesce_divider) - 1) / uint(coalesce_divider);

			uint coalesce_mult = coalesce_divider > 0 ? uint(coalesce_divider) : 1u;
			uint amplification = coalesce_divider < 0 ? uint(-coalesce_divider) : 1;

			IndirectCommands cmd;
			cmd.primary_execute = uvec3(WG_DIVIDER, total_wgs / WG_DIVIDER, amplification);
			cmd.primary_linear_offset = node_linear_offset;
			cmd.secondary_execute = uvec3(total_wgs % WG_DIVIDER, 1, amplification);
			cmd.secondary_linear_offset = node_linear_offset + cmd.primary_execute.y * WG_DIVIDER * coalesce_mult;
			cmd.end_elements = counts.total + node_linear_offset;
			cmd.linear_offset_atomic = node_linear_offset;
			cmd.total_fused_elements = counts.fused;

			cmd.expander_execute = uvec3(min(0xffffu, (counts.fused + FUSED_DIVIDER - 1) / FUSED_DIVIDER), 1, 1);

			registers.commands.indirect_commands[node_index] = cmd;

			// Reset the counters so we don't have to do an extra pass on next iteration.
			registers.atomics.node_counts[node_index] = NodeCounts(0, 0);
		}

		// Wave-uniform accumulate.
		linear_offset += total_scan;
	}

	if (subgroupAny(has_share_input))
	{
		subgroupMemoryBarrierBuffer();
		subgroupBarrier();

		// There cannot be chains of sharing, i.e. you cannot have A sharing with B, and B sharing with C,
		// so this cannot cause any weird WAR hazard.

		for (uint i = gl_SubgroupInvocationID; i < registers.num_nodes; i += gl_SubgroupSize)
		{
			uint sharing_index = registers.node_share_mapping.data[i];
			if (sharing_index != ~0u)
			{
				// We need to know the primary_linear_offset to set up the indirect properly.
				IndirectCommands other_cmd = registers.commands.indirect_commands[sharing_index];
				IndirectCommands cmd = registers.commands.indirect_commands[i];

				int coalesce_divider = registers.dividers.data[i];
				uint coalesce_mult = coalesce_divider != 0 ? coalesce_divider : 1u;
				uint amplification = coalesce_divider < 0 ? uint(-coalesce_divider) : 1;

				registers.commands.indirect_commands[i].primary_execute.z = amplification;
				registers.commands.indirect_commands[i].secondary_execute.z = amplification;
				registers.commands.indirect_commands[i].primary_linear_offset = other_cmd.primary_linear_offset;
				registers.commands.indirect_commands[i].secondary_linear_offset = other_cmd.primary_linear_offset + cmd.primary_execute.y * WG_DIVIDER * coalesce_mult;
				registers.commands.indirect_commands[i].end_elements = other_cmd.end_elements;
			}
		}
	}

	// Reset the counters so we don't have to do an extra pass on next iteration.
	if (gl_SubgroupInvocationID == 0)
		registers.atomics.payload_atomic = 0;
}
