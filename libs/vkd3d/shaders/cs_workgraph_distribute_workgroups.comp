#version 450
#extension GL_KHR_shader_subgroup_basic : require
#extension GL_KHR_shader_subgroup_ballot : require
#extension GL_KHR_shader_subgroup_arithmetic : require
#extension GL_KHR_shader_subgroup_vote : require
#extension GL_EXT_buffer_reference : require
#extension GL_GOOGLE_include_directive : require

#extension GL_KHR_memory_scope_semantics : require
#pragma use_vulkan_memory_model

layout(local_size_x_id = 0) in;
layout(constant_id = 1) const uint FUSED_DIVIDER = 0;
layout(constant_id = 2) const bool COMPACT_BROADCAST_NODES = false;

// If maxComputeWorkGroups[0] is huge, we don't have to worry.
// AMD and NV support that at least.
// When we introduce mesh shaders, we'll have to be more conservative.
// However, mesh nodes will require some esoteric MDI handling anyway, so ... eh.
layout(constant_id = 3) const bool REQUIRE_WG_DIVIDER = true;

#include "cs_workgraph_data_structures.h"

layout(buffer_reference, buffer_reference_align = 8, std430) buffer NodeAtomics
{
	uint payload_atomic;
	uint fused_atomic;
	uint node_counts[];
};

// Sharing across threads requires coherent, even within a subgroup.
layout(buffer_reference, buffer_reference_align = 16, std430) subgroupcoherent buffer IndirectCommandsBuffer
{
	uvec4 expander_execute_total_elements;
	IndirectCommands indirect_commands[];
};

layout(buffer_reference, buffer_reference_align = 4, std430) readonly buffer DividersOrAmplification
{
	int data[];
};

layout(buffer_reference, buffer_reference_align = 4, std430) readonly buffer NodeShareMapping
{
	uint data[];
};

layout(push_constant, std430) uniform Registers
{
	NodeAtomics atomics;
	IndirectCommandsBuffer commands;
	DividersOrAmplification dividers;
	NodeShareMapping node_share_mapping;
	uint num_nodes;
} registers;

// Assumption is that number of nodes is fairly small and can reasonably be iterated over one wave.
// We do very little work here, just parcel out memory regions and have fun.
// Here, we could in theory detect OOM, report the failure and nop out the indirects.

const uint WG_DIVIDER = 32 * 1024;
// Arbitrary tuneable.
const uint AMPLIFICATION_EXTRA_SHIFT = 2;

void main()
{
	uint linear_offset = 0;
	bool has_share_input = false;

	for (uint i = 0; i < registers.num_nodes; i += gl_SubgroupSize)
	{
		uint node_index = i + gl_SubgroupInvocationID;
		uint counts = 0;
		uint total_wgs;

		if (node_index < registers.num_nodes)
		{
			uint sharing_index = registers.node_share_mapping.data[node_index];
			if (sharing_index == ~0u)
			{
				counts = registers.atomics.node_counts[node_index];
				total_wgs = counts;
			}
			else
			{
				// Don't contribute to the prefix sum.
				// It is not allowed for a node to be both a real input while also sharing input of another node.
				total_wgs = registers.atomics.node_counts[sharing_index];
				has_share_input = true;
			}
		}

		uint scan = subgroupInclusiveAdd(counts);
		uint total_scan = subgroupBroadcast(scan, gl_SubgroupSize - 1);
		scan -= counts;

		if (node_index < registers.num_nodes)
		{
			uint node_linear_offset = scan + linear_offset;
			int coalesce_divider = registers.dividers.data[node_index];

			// Could make this multiplier-based if we need to.
			if (coalesce_divider > 0)
				total_wgs = (total_wgs + uint(coalesce_divider) - 1) / uint(coalesce_divider);

			uint coalesce_mult = coalesce_divider > 0 ? uint(coalesce_divider) : 1u;
			uint amplification = coalesce_divider < 0 ? uint(-coalesce_divider) : 1;

			bool dynamic_amplification = amplification <= 0xffffu;
			amplification &= 0xffffu;

			// Try to balance work we spawn on the GPU.
			amplification = max(1u, amplification >> findMSB(max(total_wgs >> AMPLIFICATION_EXTRA_SHIFT, 1u)));

			IndirectCommands cmd;
			uint spilled_wgs;

			if (REQUIRE_WG_DIVIDER)
			{
				cmd.primary_execute = uvec3(WG_DIVIDER, total_wgs / WG_DIVIDER, amplification);
				spilled_wgs = total_wgs % WG_DIVIDER;
			}
			else
			{
				cmd.primary_execute = uvec3(0);
				spilled_wgs = total_wgs;
			}

			cmd.primary_linear_offset = node_linear_offset;

			// If we only do secondary executions we can compact empty broadcasts easily.
			// This is relevant for AMD Compute Rasterizer demo for whatever reason. *shrug*
			uint secondary_executions = COMPACT_BROADCAST_NODES &&
				dynamic_amplification && coalesce_divider < 0 &&
				cmd.primary_execute.y == 0 ? 0u : spilled_wgs;

			cmd.secondary_execute = uvec3(secondary_executions, 1, amplification);
			cmd.secondary_linear_offset = node_linear_offset + cmd.primary_execute.y * WG_DIVIDER * coalesce_mult;
			cmd.end_elements = counts + node_linear_offset;
			cmd.linear_offset_atomic = node_linear_offset;

			registers.commands.indirect_commands[node_index] = cmd;

			// Reset the counters so we don't have to do an extra pass on next iteration.
			registers.atomics.node_counts[node_index] = 0;
		}

		// Wave-uniform accumulate.
		linear_offset += total_scan;
	}

	if (subgroupAny(has_share_input))
	{
		subgroupMemoryBarrierBuffer();
		subgroupBarrier();

		// There cannot be chains of sharing, i.e. you cannot have A sharing with B, and B sharing with C,
		// so this cannot cause any weird WAR hazard.

		for (uint i = gl_SubgroupInvocationID; i < registers.num_nodes; i += gl_SubgroupSize)
		{
			uint sharing_index = registers.node_share_mapping.data[i];
			if (sharing_index != ~0u)
			{
				// We need to know the primary_linear_offset to set up the indirect properly.
				IndirectCommands other_cmd = registers.commands.indirect_commands[sharing_index];
				IndirectCommands cmd = registers.commands.indirect_commands[i];

				int coalesce_divider = registers.dividers.data[i];
				uint coalesce_mult = coalesce_divider != 0 ? coalesce_divider : 1u;
				uint amplification = coalesce_divider < 0 ? uint(-coalesce_divider) : 1;

				amplification &= 0xffffu;

				uint total_wgs = other_cmd.end_elements - other_cmd.primary_linear_offset;
				// Try to balance work we spawn on the GPU.
				amplification = max(1u, amplification >> findMSB(max(total_wgs >> AMPLIFICATION_EXTRA_SHIFT, 1u)));

				registers.commands.indirect_commands[i].primary_execute.z = amplification;
				registers.commands.indirect_commands[i].secondary_execute.z = amplification;
				registers.commands.indirect_commands[i].primary_linear_offset = other_cmd.primary_linear_offset;
				registers.commands.indirect_commands[i].secondary_linear_offset = other_cmd.primary_linear_offset + cmd.primary_execute.y * WG_DIVIDER * coalesce_mult;
				registers.commands.indirect_commands[i].end_elements = other_cmd.end_elements;
			}
		}
	}

	// Reset the counters so we don't have to do an extra pass on next iteration.
	// Also, have a single thread emit the fused expander kernel.
	if (subgroupElect())
	{
		uint fused = registers.atomics.fused_atomic;

		const uint FUSE_LIMIT = REQUIRE_WG_DIVIDER ? 0xffffu : 0xffffffu;

		// Do a single ubershader that does payload expansion. Goes as wide as reasonably possible.
		registers.commands.expander_execute_total_elements =
			uvec4(min(FUSE_LIMIT, (fused + FUSED_DIVIDER - 1) / FUSED_DIVIDER), 1, 1, fused);

		registers.atomics.payload_atomic = 0;
		registers.atomics.fused_atomic = 0;
	}
}
